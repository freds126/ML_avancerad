{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import scipy.special as sp_spec\n",
    "import scipy.stats as sp_stats\n",
    "import numpy.random as np_rand\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import seaborn as sns\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Bernoulli Mixture Model\n",
    "See the solution pdf for derivation of CAVI updates, ELBO and for notation used throughout the notebook. See theory part of Exercise problems for algorithm psuedo code and more. Updates are written using for-loops to more clearly show relation to derivations, however, you are recommended to use matrix operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_and_priors(N: int, D: int, K: int, theta_a: float, theta_b: float, pi_alpha: List[int]):\n",
    "  \"\"\"\n",
    "  Generates datapoints and hidden variables given parameters for the priors.\n",
    "  :param N: Number of datapoints\n",
    "  :param D: Data dimension.\n",
    "  :param K: Number of mixture components.\n",
    "  :param theta_a: \"a\" parameter of the Beta(a,b) prior on theta.\n",
    "  :param theta_b: \"b\" parameter of the Beta(a,b) prior on theta.\n",
    "  :param pi_alpha: parameter of the Dirichlet(alpha) prior on pi.\n",
    "  :return:\n",
    "  \"\"\"\n",
    "  theta = np_rand.beta(theta_a, theta_b, size=(K,D))  # K x D matrix\n",
    "  pi = np_rand.dirichlet(pi_alpha) # size K array\n",
    "  x = np.zeros((N,D)) # N x D matrix\n",
    "  z = np.zeros(N)     # size N array \n",
    "  for n in range(N):\n",
    "    z_n = np_rand.multinomial(1, pi).argmax() # for each datapoint n, sample the component assignment/class variable\n",
    "    x_n = np.zeros(D)\n",
    "    for d in range(D): \n",
    "      x_n[d] = np_rand.binomial(1, theta[z_n, d]) # Sample x_nd from Bernoulli(theta_kd)\n",
    "    z[n] = z_n\n",
    "    x[n, :] = x_n\n",
    "\n",
    "  return x, z, theta, pi\n",
    "\n",
    "def generate_data(N: int, D: int, K: int, theta: np.ndarray , pi: np.ndarray):\n",
    "  \"\"\"\n",
    "   Generates datapoints given values of the hidden variables. Can be used for more \"control\" of experiments.\n",
    "  :param N: Number of datapoints\n",
    "  :param D: Data dimension.\n",
    "  :param K: Number of mixture components.\n",
    "  :param theta: Values of theta.\n",
    "  :param pi: Value of pi.\n",
    "  :return:\n",
    "  \"\"\"\n",
    "  x = np.zeros((N,D)) # N x D matrix\n",
    "  z = np.zeros(N)     # size N array\n",
    "  for n in range(N):\n",
    "    z_n = np_rand.multinomial(1, pi).argmax() # for each datapoint n, sample the component assignment/class variable\n",
    "    x_n = np.zeros(D)\n",
    "    for d in range(D):\n",
    "      x_n[d] = np_rand.binomial(1, theta[z_n, d])\n",
    "    z[n] = z_n\n",
    "    x[n, :] = x_n\n",
    "\n",
    "  return x, z, theta, pi\n",
    "\n",
    "# Test the functions by running for some simple cases and verify that data and variables look as expected.\n",
    "N = 1000\n",
    "D = 3\n",
    "K = 2\n",
    "\n",
    "theta = np.array([[0.1, 0.7, 0.2], [0.6, 0.2, 0.2]])\n",
    "pi = np.array([0.4, 0.6])\n",
    "x, z, theta, pi = generate_data(N, D, K, theta, pi)\n",
    "print(f\"First 3 datapoints: {x[0:3, 0:5]}\")\n",
    "print(f\"First 3 z: {z[0:3]}\")\n",
    "\n",
    "x_mean = np.mean(x, axis=0)\n",
    "print(f\"Mean of x: {x_mean}\")\n",
    "print(f\"Expected x: {pi[0] * theta[0] + pi[1] * theta[1]}\")\n",
    "z_one_hot = np.eye(K)[z.astype(int)]\n",
    "print(f\"Mean of z: {np.mean(z_one_hot, axis=0)}\")\n",
    "print(f\"Expected z: {pi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pandas as pd # Using pandas for easy counting and plotting\n",
    "\n",
    "# --- This code cell visualizes the data generated in the cell above ---\n",
    "\n",
    "# We will use the variables: x, z, theta, pi, K, D, N\n",
    "\n",
    "# 1. Create the figure with 3 subplots\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "fig.suptitle('Bernoulli Mixture Model Data Visualization', fontsize=20)\n",
    "\n",
    "# 2. Plot True Mixture Weights (pi)\n",
    "ax_pi = fig.add_subplot(2, 2, 1)\n",
    "colors = plt.cm.jet(np.linspace(0, 1, K))\n",
    "ax_pi.bar(range(K), pi, color=colors, alpha=0.8)\n",
    "ax_pi.set_title('True Mixture Weights ($\\pi$)', fontsize=14)\n",
    "ax_pi.set_xlabel('Cluster (k)', fontsize=12)\n",
    "ax_pi.set_ylabel('Probability', fontsize=12)\n",
    "ax_pi.set_xticks(range(K))\n",
    "ax_pi.set_ylim(0, 1.0)\n",
    "for i, v in enumerate(pi):\n",
    "    ax_pi.text(i, v + 0.02, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Plot True Cluster Probabilities (theta)\n",
    "ax_theta = fig.add_subplot(2, 2, 2)\n",
    "im = ax_theta.imshow(theta, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax_theta.set_title('True Cluster Probabilities ($\\theta$)', fontsize=14)\n",
    "ax_theta.set_xlabel('Feature (d)', fontsize=12)\n",
    "ax_theta.set_ylabel('Cluster (k)', fontsize=12)\n",
    "ax_theta.set_xticks(range(D))\n",
    "ax_theta.set_yticks(range(K))\n",
    "# Add text labels for the probabilities\n",
    "for k in range(K):\n",
    "    for d in range(D):\n",
    "        color = 'white' if theta[k, d] < 0.5 else 'black'\n",
    "        ax_theta.text(d, k, f'{theta[k, d]:.2f}', ha='center', va='center', color=color, fontweight='bold')\n",
    "fig.colorbar(im, ax=ax_theta, label='P(x_d = 1 | z=k)', fraction=0.046, pad=0.04)\n",
    "\n",
    "\n",
    "# 4. Plot the distribution of the generated data (x)\n",
    "ax_data = fig.add_subplot(2, 1, 2)\n",
    "\n",
    "# To visualize the data, we will count the occurrences of each unique binary pattern\n",
    "# and group them by their true cluster assignment (z)\n",
    "\n",
    "# Create a DataFrame for easy grouping\n",
    "df = pd.DataFrame(x, columns=[f'd{i}' for i in range(D)])\n",
    "df['z'] = z.astype(int)\n",
    "\n",
    "# Create a 'pattern' column (e.g., '010') for easy counting\n",
    "df['pattern'] = df.apply(lambda row: ''.join(row.iloc[:D].astype(int).astype(str)), axis=1)\n",
    "\n",
    "# Count occurrences of each pattern, grouped by cluster\n",
    "counts = df.groupby('pattern')['z'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Ensure all 2^D patterns are present, even if count is 0\n",
    "all_patterns = [''.join(map(str, p)) for p in product([0, 1], repeat=D)]\n",
    "counts = counts.reindex(all_patterns, fill_value=0)\n",
    "counts.sort_index(inplace=True)\n",
    "\n",
    "# Plot the grouped bar chart\n",
    "counts.plot(kind='bar', ax=ax_data, color=colors, alpha=0.8, rot=0)\n",
    "\n",
    "ax_data.set_title(f'Distribution of {N} Simulated Data Points (D={D})', fontsize=16)\n",
    "ax_data.set_xlabel('Binary Pattern (d0, d1, d2)', fontsize=12)\n",
    "ax_data.set_ylabel('Count', fontsize=12)\n",
    "ax_data.legend(title='True Cluster (z)', loc='upper left')\n",
    "ax_data.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for c in ax_data.containers:\n",
    "    ax_data.bar_label(c, label_type='edge', fontsize=9, padding=2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust for main title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAVI Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAVI updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $q(\\pi)$ update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_pi(E_Z, alpha_prior):\n",
    "    alpha_star = np.sum(E_Z, axis=0) + alpha_prior  # size K array\n",
    "    return alpha_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $q(\\theta)$ update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_theta(x, r_q, a_prior, b_prior):\n",
    "    E_Z = r_q\n",
    "    N, D = x.shape\n",
    "    K = r_q.shape[1]\n",
    "\n",
    "    # With Einsum\n",
    "    a_star = np.einsum('nk,nd->kd', E_Z, x) + a_prior\n",
    "    b_star = np.einsum('nk,nd->kd', E_Z, 1 - x) + b_prior\n",
    "\n",
    "    # Without Einsum\n",
    "    # a_star = np.zeros((K, D))\n",
    "    # b_star = np.zeros((K, D))\n",
    "    # for k in range(K):\n",
    "    #     for d in range(D):\n",
    "    #         a_kd = np.sum(E_Z[:,k] * x[:,d]) + a_prior\n",
    "    #         b_kd = np.sum(E_Z[:,k] * (1 - x[:, d])) + b_prior\n",
    "    #         a_star[k, d] = a_kd\n",
    "    #         b_star[k, d] = b_kd\n",
    "\n",
    "    return a_star, b_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $q(Z)$ update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_Z(x, a_q, b_q, alpha_q):\n",
    "    \"\"\"\n",
    "    Implements the CAVI update equation of q(Z) derived in the solution of the exercise.\n",
    "    Returns: r_star: N x K matrix\n",
    "    \"\"\"\n",
    "    E_log_theta = sp_spec.digamma(a_q) - sp_spec.digamma(a_q + b_q)         # K x D\n",
    "    E_log_1_minus_theta = sp_spec.digamma(b_q) - sp_spec.digamma(a_q + b_q) # K x D\n",
    "    E_log_pi = sp_spec.digamma(alpha_q) - sp_spec.digamma(np.sum(alpha_q))  # K array\n",
    "\n",
    "    log_rho = (np.einsum('nd,kd->nk', x, E_log_theta)\n",
    "               + np.einsum('nd,kd->nk', 1 - x, E_log_1_minus_theta)\n",
    "               + E_log_pi)\n",
    "\n",
    "    r_star = np.exp(log_rho - sp_spec.logsumexp(log_rho, axis=1, keepdims=True))\n",
    "    return r_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO closed form calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_beta_function(a):\n",
    "    eps = 0.0001  # for numerical stability to avoid -inf\n",
    "    return np.exp(np.sum(sp_spec.gammaln(a + eps) - sp_spec.gammaln(np.sum(a + eps))))\n",
    "\n",
    "def calculate_elbo(x, r_q, a_q, b_q, alpha_q):\n",
    "    N, D = x.shape\n",
    "    K = r_q.shape[1]\n",
    "    E_log_theta = sp_spec.digamma(a_q) - sp_spec.digamma(a_q + b_q)\n",
    "    E_log_1_minus_theta = sp_spec.digamma(b_q) - sp_spec.digamma(a_q + b_q)\n",
    "    E_log_pi = sp_spec.digamma(alpha_q) - sp_spec.digamma(np.sum(alpha_q))\n",
    "    alpha_0 = np.sum(alpha_q)\n",
    "    CR_likelihood = np.sum(r_q * (np.einsum('nd,kd->nk', x, E_log_theta)\n",
    "                                  + np.einsum('nd,kd->nk', 1 - x, E_log_1_minus_theta)))\n",
    "    CR_Z = np.sum(r_q * E_log_pi)\n",
    "    eps = 0.0001  # for numerical stability to avoid -inf\n",
    "    CR_pi = -np.log(multivariate_beta_function(alpha_q) + eps) + np.sum((alpha_q - 1) * E_log_pi)\n",
    "    CR_theta = np.sum((a_q-1) * E_log_theta + (b_q-1) * E_log_1_minus_theta - sp_spec.beta(a_q, b_q))\n",
    "    H_Z = np.sum(r_q * np.log(r_q + eps))\n",
    "    H_pi = np.log(multivariate_beta_function(alpha_q) + eps) + (alpha_0 - K) * sp_spec.digamma(alpha_0) - \\\n",
    "           np.sum((alpha_q - 1) * sp_spec.digamma(alpha_q))\n",
    "    Beta_ab = sp_spec.beta(a_q, b_q) + eps\n",
    "    H_theta = np.sum(np.log(Beta_ab) - (a_q - 1) * sp_spec.digamma(a_q) -\n",
    "                     (b_q - 1) * sp_spec.digamma(b_q) + (a_q + b_q + 2) * sp_spec.digamma(a_q + b_q))\n",
    "    elbo = CR_likelihood + CR_Z + CR_pi + CR_theta + H_Z + H_pi + H_theta\n",
    "    return elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q(x, K):\n",
    "    #r_q_init = np.random.random((N,K))\n",
    "    N, D = x.shape\n",
    "    kmeans =  KMeans(n_clusters=K, random_state=0, n_init=30).fit(x)\n",
    "    labels = kmeans.labels_\n",
    "    a_q_init = np.zeros((K, D))\n",
    "    b_q_init = np.zeros((K, D))\n",
    "    for k in range(K):\n",
    "        x_k = x[labels == k]\n",
    "        a_q_init[k, :] = np.mean(x_k, axis=0) + 1\n",
    "        b_q_init[k, :] = np.mean(1 - x_k, axis=0) + 1\n",
    "\n",
    "    r_q_init = np.random.rand(N, K)\n",
    "    r_q_init = r_q_init / np.sum(r_q_init, axis=1, keepdims=True)  # normalize\n",
    "    a_q_init = np.random.randint(1, 100, size=(K,D))\n",
    "    b_q_init = np.random.randint(1, 100, size=(K,D))\n",
    "    alpha_q_init = np.ones(K)\n",
    "    return r_q_init, a_q_init, b_q_init, alpha_q_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAVI algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAVI_algorithm(x, K, n_iter, a_prior, b_prior, alpha_prior, step_size=0.01, tol=1e-6):\n",
    "    N, D = x.shape\n",
    "\n",
    "    r_q, a_q, b_q, alpha_q = initialize_q(x, K)\n",
    "    elbo_after_init = calculate_elbo(x, r_q, a_q, b_q, alpha_q)\n",
    "    # Store output per iteration\n",
    "    elbo = [elbo_after_init]\n",
    "    r_q_out = [r_q]\n",
    "    a_q_out = [a_q]\n",
    "    b_q_out = [b_q]\n",
    "    alpha_q_out = [alpha_q]\n",
    "\n",
    "    pbar = tqdm.tqdm(range(n_iter))\n",
    "    for i in pbar:\n",
    "        # CAVI updates\n",
    "        # q(Z) update\n",
    "        r_star = update_q_Z(x, a_q, b_q, alpha_q)\n",
    "        r_q += step_size * (r_star - r_q)\n",
    "        r_q = r_q / np.sum(r_q, axis=1, keepdims=True)  # normalize\n",
    "\n",
    "        # q(pi) update\n",
    "        alpha_star = update_q_pi(r_q, alpha_prior)\n",
    "        alpha_q += step_size * (alpha_star - alpha_q)\n",
    "\n",
    "        # q(theta) update\n",
    "        a_star, b_star = update_q_theta(x, r_q, a_prior, b_prior)\n",
    "        a_q = a_q + step_size * (a_star - a_q)\n",
    "        b_q = b_q + step_size * (b_star - b_q)\n",
    "\n",
    "        # ELBO\n",
    "        elbo.append(calculate_elbo(x, r_q, a_q, b_q, alpha_q))\n",
    "\n",
    "        # outputs\n",
    "        r_q_out.append(r_q)\n",
    "        a_q_out.append(a_q)\n",
    "        b_q_out.append(b_q)\n",
    "        alpha_q_out.append(alpha_q)\n",
    "\n",
    "        pbar.set_description(f\"ELBO: {elbo[i]:.2f}\")\n",
    "\n",
    "        if i > 1 and np.abs(elbo[i] - elbo[i-1]) < tol:\n",
    "            break\n",
    "\n",
    "    r_q_out = np.array(r_q_out)\n",
    "    a_q_out = np.array(a_q_out)\n",
    "    b_q_out = np.array(b_q_out)\n",
    "    alpha_q_out = np.array(alpha_q_out)\n",
    "    elbo = np.array(elbo)\n",
    "    out = {\"r_q\": r_q_out, \"a_q\": a_q_out, \"b_q\": b_q_out, \"alpha_q\": alpha_q_out, \"elbo\": elbo}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run optimization on simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "D = 100\n",
    "K = 5\n",
    "n_iter = 3000\n",
    "a_prior = 1.\n",
    "b_prior = 1.\n",
    "alpha_prior = np.ones(K) * 1.0\n",
    "# theta = np.array([[0.1, 0.9, 0.0, 0.0], [0.0, 0.05, 0.45, 0.5]])\n",
    "# pi = np.array([0.6, 0.4])\n",
    "# x, z, theta, pi = generate_data(N, D, K, theta, pi)\n",
    "x, z, theta, pi = generate_data_and_priors(N, D, K, a_prior, b_prior, alpha_prior)\n",
    "out = CAVI_algorithm(x, K, n_iter, a_prior, b_prior, alpha_prior, step_size=0.01, tol=1e-6)\n",
    "r_q_out = out[\"r_q\"]\n",
    "a_q_out = out[\"a_q\"]\n",
    "b_q_out = out[\"b_q\"]\n",
    "alpha_q_out = out[\"alpha_q\"]\n",
    "elbo = out[\"elbo\"]\n",
    "np.printoptions(precision=2)\n",
    "print(f\"Print results (check for label switching).\")\n",
    "print(f\"z :{z[0:10]}\")\n",
    "# As q(Z) is our variational posterior and is a Categorical, argmax corresponds to our MAP estimates\n",
    "MAP_assignments = r_q_out[-1].argmax(axis=1)\n",
    "print(f\"r_q :{MAP_assignments[0:10]}\")\n",
    "print(f\"ARI :{adjusted_rand_score(z, MAP_assignments)}\")  # clustering metric that is independent of label switching\n",
    "\n",
    "print(f\"Expected value pi: {alpha_q_out[-1] / np.sum(alpha_q_out[-1])}\")\n",
    "print(f\"True pi: {pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo = out[\"elbo\"]\n",
    "plt.plot(elbo)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"ELBO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Visualize optimization\n",
    "\n",
    "Plots the parameters of $q(\\theta_{kd})$ for a set of k and a particular d for different iterations.\n",
    "\n",
    "The corresponding Beta distribution is also plotted for the same parameters.\n",
    "\n",
    "This is to illustrate when we are updating the parameters during the CAVI optimization, we are implicitly updating the optimal $q(\\theta_{dk})$ distribution that we derived on the board.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xlim_a_min = a_q_out.min()\n",
    "xlim_a_max = a_q_out.max()\n",
    "ylim_b_min = b_q_out.min()\n",
    "ylim_b_max = b_q_out.max()\n",
    "iterations_to_plot = [0, 10, -1]\n",
    "n_iter_to_plot = len(iterations_to_plot)\n",
    "fig, axs = plt.subplots(2, n_iter_to_plot, figsize=(10, 6))\n",
    "d_to_plot = 0  # a_kd\n",
    "k_to_plot = range(0, K)\n",
    "for i in range(0, n_iter_to_plot):\n",
    "    j = iterations_to_plot[i]\n",
    "    axs[0, i].scatter(a_q_out[j, k_to_plot, d_to_plot], b_q_out[j, k_to_plot, d_to_plot])\n",
    "    axs[0, i].set_title(f\"q(theta) a, b: {j}\")\n",
    "    axs[0, i].set_xlim(int(xlim_a_min) - 100, int(xlim_a_max) + 100)\n",
    "    axs[0, i].set_ylim(int(ylim_b_min) - 100, int(ylim_b_max) + 100)\n",
    "    \n",
    "    for k in k_to_plot:\n",
    "        q_theta_kd = sp_stats.beta(a_q_out[i, k, d_to_plot], b_q_out[i, k, d_to_plot])\n",
    "        pi_axis = np.linspace(0, 1, 100)\n",
    "        axs[1, i].plot(pi_axis, q_theta_kd.pdf(pi_axis))\n",
    "\n",
    "    if i == 0:\n",
    "        axs[0, i].set_xlabel('a')\n",
    "        axs[0, i].set_ylabel('b')\n",
    "        axs[1, i].set_ylabel('N samples')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML  # For display in Jupyter/Colab\n",
    "\n",
    "# --- 1. Create Mock Data (Replace this with your data) ---\n",
    "# This simulates 50 iterations of CAVI for K=3 clusters and d=1 feature.\n",
    "# We assume the parameters (a, b) start at (1, 1) and converge.\n",
    "n_iterations = 50\n",
    "K = 3  # Number of clusters\n",
    "d_features = 1  # Number of features\n",
    "\n",
    "# Define convergence targets for our 3 clusters\n",
    "a_targets = np.array([50.0, 10.0, 80.0])\n",
    "b_targets = np.array([20.0, 70.0, 40.0])\n",
    "\n",
    "# Create empty arrays\n",
    "a_q_out = np.zeros((n_iterations, K, d_features))\n",
    "b_q_out = np.zeros((n_iterations, K, d_features))\n",
    "\n",
    "# Fill with data converging from (1, 1) to the targets\n",
    "for k in range(K):\n",
    "    # Use linspace for a smooth convergence path\n",
    "    a_path = np.linspace(1.0, a_targets[k], n_iterations)\n",
    "    b_path = np.linspace(1.0, b_targets[k], n_iterations)\n",
    "\n",
    "    # Add some random noise to make it look like a real optimization\n",
    "    a_noise = np.random.randn(n_iterations) * (n_iterations - np.arange(n_iterations)) / n_iterations * 0.5\n",
    "    b_noise = np.random.randn(n_iterations) * (n_iterations - np.arange(n_iterations)) / n_iterations * 0.5\n",
    "\n",
    "    a_q_out[:, k, 0] = a_path + a_noise\n",
    "    b_q_out[:, k, 0] = b_path + b_noise\n",
    "\n",
    "# Ensure parameters are > 0 (Beta params must be positive)\n",
    "a_q_out[a_q_out < 0.1] = 0.1\n",
    "b_q_out[b_q_out < 0.1] = 0.1\n",
    "# --- End of Mock Data Section ---\n",
    "\n",
    "\n",
    "# --- 2. Setup Animation Constants and Data ---\n",
    "d_to_plot = 0  # The feature dimension to plot (same as your script)\n",
    "K = a_q_out.shape[1] # Number of clusters\n",
    "n_iterations = a_q_out.shape[0] # Total number of iterations\n",
    "k_to_plot = range(0, K)\n",
    "\n",
    "# Generate distinct colors for each cluster\n",
    "colors = plt.cm.jet(np.linspace(0, 1, K))\n",
    "pi_axis = np.linspace(0, 1, 100) # Axis for plotting PDFs\n",
    "\n",
    "# --- 3. Calculate Global Axis Limits (for stable animation) ---\n",
    "xlim_a_min = a_q_out[:, :, d_to_plot].min()\n",
    "xlim_a_max = a_q_out[:, :, d_to_plot].max()\n",
    "ylim_b_min = b_q_out[:, :, d_to_plot].min()\n",
    "ylim_b_max = b_q_out[:, :, d_to_plot].max()\n",
    "\n",
    "# Add 10% padding\n",
    "a_pad = (xlim_a_max - xlim_a_min) * 0.1\n",
    "b_pad = (ylim_b_max - ylim_b_min) * 0.1\n",
    "\n",
    "xlim_a = (xlim_a_min - a_pad, xlim_a_max + a_pad)\n",
    "ylim_b = (ylim_b_min - b_pad, ylim_b_max + b_pad)\n",
    "\n",
    "# --- 4. Setup the Figure ---\n",
    "# We now use a 2x1 grid, as we are animating one state over time\n",
    "fig, (ax_params, ax_pdf) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "plt.tight_layout(pad=4.0) # Add padding for titles\n",
    "\n",
    "# --- 5. Create Empty Plot \"Artists\" ---\n",
    "# We create the plot elements once, and the animation will just update their data.\n",
    "# This is much faster than clearing and redrawing.\n",
    "\n",
    "# Top plot (Parameters)\n",
    "param_trails = [] # The lines for the history\n",
    "param_heads = []  # The scatter points for the current position\n",
    "for k in k_to_plot:\n",
    "    # Trail line\n",
    "    line, = ax_params.plot([], [], marker='o', markersize=2, alpha=0.3, color=colors[k], label=f'Cluster {k}')\n",
    "    param_trails.append(line)\n",
    "    # Head point\n",
    "    head, = ax_params.plot([], [], marker='o', markersize=8, color=colors[k], markeredgecolor='black')\n",
    "    param_heads.append(head)\n",
    "\n",
    "ax_params.set_xlim(xlim_a)\n",
    "ax_params.set_ylim(ylim_b)\n",
    "ax_params.set_xlabel('Parameter a')\n",
    "ax_params.set_ylabel('Parameter b')\n",
    "ax_params.set_title('q(theta) Parameter Convergence')\n",
    "ax_params.legend()\n",
    "ax_params.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Bottom plot (PDFs)\n",
    "pdf_lines = []\n",
    "for k in k_to_plot:\n",
    "    line, = ax_pdf.plot([], [], color=colors[k], lw=2, label=f'Cluster {k}')\n",
    "    pdf_lines.append(line)\n",
    "\n",
    "ax_pdf.set_xlim(0, 1)\n",
    "# We'll set the Y-limit dynamically in the update function, or you can find the global max\n",
    "ax_pdf.set_xlabel('$\\pi$ value') # Use LaTeX for pi\n",
    "ax_pdf.set_ylabel('Probability Density')\n",
    "ax_pdf.set_title('q(theta) PDF')\n",
    "ax_pdf.legend()\n",
    "ax_pdf.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Global iteration title\n",
    "iter_title = fig.suptitle('Iteration 0', fontsize=16)\n",
    "\n",
    "# --- 6. Define Animation Update Function ---\n",
    "# This function is called for each frame (iteration)\n",
    "def update(i):\n",
    "    max_pdf_y = 0 # To dynamically set Y-limit for the PDF plot\n",
    "\n",
    "    for k in k_to_plot:\n",
    "        # --- Update Top Plot (Parameters) ---\n",
    "\n",
    "        # Get history up to current iteration i\n",
    "        a_history = a_q_out[0:i+1, k, d_to_plot]\n",
    "        b_history = b_q_out[0:i+1, k, d_to_plot]\n",
    "\n",
    "        # Update the trail line\n",
    "        param_trails[k].set_data(a_history, b_history)\n",
    "\n",
    "        # Update the head point (THIS IS THE CORRECTED LINE)\n",
    "        param_heads[k].set_data([a_history[-1]], [b_history[-1]])\n",
    "\n",
    "        # --- Update Bottom Plot (PDFs) ---\n",
    "\n",
    "        # Get current parameters\n",
    "        a_current = a_q_out[i, k, d_to_plot]\n",
    "        b_current = b_q_out[i, k, d_to_plot]\n",
    "\n",
    "        # Calculate new PDF\n",
    "        q_theta_kd = sp_stats.beta(a_current, b_current)\n",
    "        pdf_values = q_theta_kd.pdf(pi_axis)\n",
    "\n",
    "        # Update the PDF line\n",
    "        pdf_lines[k].set_data(pi_axis, pdf_values)\n",
    "\n",
    "        # Track the max PDF height to adjust the y-axis\n",
    "        if np.any(pdf_values > max_pdf_y):\n",
    "            max_pdf_y = np.max(pdf_values[np.isfinite(pdf_values)]) # Ignore inf\n",
    "\n",
    "    # Update the PDF y-axis limit\n",
    "    ax_pdf.set_ylim(0, max_pdf_y * 1.1)\n",
    "\n",
    "    # Update the main title\n",
    "    iter_title.set_text(f'CAVI Iteration {i}')\n",
    "\n",
    "    # Return all the artists that were updated\n",
    "    return param_trails + param_heads + pdf_lines + [iter_title]\n",
    "\n",
    "# --- 7. Create and Display Animation ---\n",
    "\n",
    "# Create the animation object\n",
    "# interval=100 means 100ms per frame (10 FPS)\n",
    "# blit=False is often more stable, though blit=True is faster if it works\n",
    "ani = FuncAnimation(fig, update, frames=n_iterations,\n",
    "                    interval=100, blit=False)\n",
    "\n",
    "# To display in Jupyter Notebook or Google Colab:\n",
    "plt.close(fig) # Prevent static plot from showing\n",
    "HTML(ani.to_jshtml())\n",
    "\n",
    "# To display in a standard Python script, uncomment the following line:\n",
    "# plt.show()\n",
    "\n",
    "# To save the animation as a GIF (requires 'imagemagick' or 'pillow'):\n",
    "# print(\"Saving animation... (this may take a moment)\")\n",
    "# ani.save('cavi_animation.gif', writer='pillow', fps=10)\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "X = X.to_numpy()\n",
    "\n",
    "selected_digits = [0, 1, 2, 3, 4]\n",
    "train_filter = []\n",
    "N_k = [1000, 1000, 1000, 1000, 1000]\n",
    "for k,s in enumerate(selected_digits):\n",
    "    train_filter.append(np.where(y == str(s))[0][0:N_k[k]])\n",
    "\n",
    "train_filter = np.concatenate(train_filter)\n",
    "X = X[train_filter, :]\n",
    "y = y[train_filter]\n",
    "N = len(train_filter)\n",
    "X = X[0:N, :]\n",
    "X = (X > 127.5).astype(int)  # binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = X.shape\n",
    "K = len(selected_digits)\n",
    "n_iter = 500\n",
    "a_prior = 1.\n",
    "b_prior = 1.\n",
    "alpha_prior = np.ones(K) * N/K\n",
    "\n",
    "print(f\"Priors: {alpha_prior}\")\n",
    "print(f\"Number of iterations: {n_iter}\")\n",
    "print(f\"Number of datapoints: {N}\")\n",
    "out = CAVI_algorithm(X, K, n_iter,\n",
    "                     a_prior, b_prior, alpha_prior,\n",
    "                     step_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo = out[\"elbo\"]\n",
    "plt.plot(elbo)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"ELBO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_q_out = out[\"r_q\"]\n",
    "a_q_out = out[\"a_q\"]\n",
    "b_q_out = out[\"b_q\"]\n",
    "alpha_q_out = out[\"alpha_q\"]\n",
    "elbo = out[\"elbo\"]\n",
    "np.printoptions(precision=2)\n",
    "\n",
    "sklearn_kmeans = KMeans(n_clusters=K, random_state=0, n_init=30).fit(X)\n",
    "labels = sklearn_kmeans.labels_\n",
    "print(f\"ARI KMeans:{adjusted_rand_score(y[0:N], labels)}\")  # clustering metric that is independent of label switching\n",
    "\n",
    "MAP_assignments = r_q_out[-1].argmax(axis=1)\n",
    "print(f\"ARI CAVI:{adjusted_rand_score(y[0:N], MAP_assignments)}\")\n",
    "\n",
    "y_labels, y_counts = np.unique(y[0:N], return_counts=True)\n",
    "print(f\"True pi: {y_counts / np.sum(y_counts)}\")\n",
    "print(f\"Expected value pi: {alpha_q_out[-1] / np.sum(alpha_q_out[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    plt.subplot(2, int(K/2) + (K % 2), k + 1)\n",
    "    E_theta_k = a_q_out[-1][k] / (a_q_out[-1][k] + b_q_out[-1][k])\n",
    "    plt.imshow(E_theta_k.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize learned cluster parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML  # For display in Jupyter/Colab\n",
    "\n",
    "# --- Assume a_q_out and b_q_out are populated ---\n",
    "# Example dummy data (replace with your actual data):\n",
    "# import numpy as np\n",
    "# L_iterations = 50  # Number of iterations\n",
    "# K_components = 10  # Number of components\n",
    "# a_q_out = [np.random.rand(K_components, 28*28) for _ in range(L_iterations)]\n",
    "# b_q_out = [np.random.rand(K_components, 28*28) for _ in range(L_iterations)]\n",
    "# -------------------------------------------------\n",
    "\n",
    "# 1. Get total iterations (L) and components (K)\n",
    "L = 200 #len(a_q_out)\n",
    "K = len(a_q_out[0])\n",
    "\n",
    "# 2. Set up the figure and subplot grid\n",
    "# We create a 2-row grid, calculating columns needed\n",
    "nrows = 2\n",
    "ncols = (K + 1) // 2  # This is a cleaner way to get (K/2) rounded up\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 2, nrows * 2 + 0.5))\n",
    "axes_flat = axes.flatten() # Flatten grid for easy indexing\n",
    "\n",
    "# Turn off any extra, unused subplots\n",
    "for k in range(K, nrows * ncols):\n",
    "    axes_flat[k].axis(\"off\")\n",
    "\n",
    "# 3. Define the update function (called for each frame)\n",
    "def update(i):\n",
    "    # 'i' is the frame number, which we use as the iteration index\n",
    "    for k in range(K):\n",
    "        ax = axes_flat[k]\n",
    "        ax.clear()  # Clear the axis for the new frame\n",
    "\n",
    "        # Calculate E_theta_k for iteration 'i' and component 'k'\n",
    "        a_k_i = a_q_out[i][k]\n",
    "        b_k_i = b_q_out[i][k]\n",
    "        E_theta_k = a_k_i / (a_k_i + b_k_i)\n",
    "\n",
    "        # Plot the image\n",
    "        ax.imshow(E_theta_k.reshape(28, 28), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        ax.set_title(f\"k = {k}\", fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # Add a title to the whole figure showing the iteration\n",
    "    fig.suptitle(f\"Iteration {i + 1} / {L}\", fontsize=14)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle\n",
    "\n",
    "# 4. Create the animation\n",
    "# interval=200 means 200ms per frame\n",
    "ani = FuncAnimation(fig, update, frames=L, interval=200, blit=False)\n",
    "\n",
    "# 5. Display the animation (in Jupyter/Colab)\n",
    "plt.close(fig) # Prevent the static plot from showing\n",
    "#HTML(ani.to_html5_video())\n",
    "HTML(ani.to_jshtml())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot most certain vs uncertain images\n",
    "fig, axs = plt.subplots(2, K, figsize=(20, 10))\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "for k in range(K):\n",
    "    idx_qZ_k = np.where(MAP_assignments==k)[0]\n",
    "    max_entropy_idx = np.argmax(sp_stats.entropy(r_q_out[-1][idx_qZ_k], axis=1))\n",
    "    print(f\"Probability of most uncertain sample of cluster {k}: {r_q_out[-1][idx_qZ_k][max_entropy_idx]}\")\n",
    "    min_entropy_idx = np.argmin(sp_stats.entropy(r_q_out[-1][idx_qZ_k], axis=1))\n",
    "    print(f\"Probability of most certain sample of cluster {k}: {r_q_out[-1][idx_qZ_k][min_entropy_idx]}\")\n",
    "    max_entr_img = X[idx_qZ_k][max_entropy_idx]\n",
    "    min_entr_img = X[idx_qZ_k][min_entropy_idx]\n",
    "    axs[0, k].imshow(max_entr_img.reshape(28, 28), cmap=\"gray\")\n",
    "    axs[1, k].imshow(min_entr_img.reshape(28, 28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
