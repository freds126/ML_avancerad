{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.special as sp_spec\n",
    "import scipy.stats as sp_stats\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Assignment 2E - LDA with SVI\n",
    "In this assignment, you will implement Stochastic Variational Inference (SVI) for Latent Dirichlet Allocation (LDA) using the Coordinate Ascent Variational Inference (CAVI) updates as a template.\n",
    "\n",
    "Your SVI implementation should be based on the CAVI implementation provided. You will need to modify the local updates to be performed on a mini-batch of documents and then update the global parameters accordingly.\n",
    "\n",
    "For these dataset, do not expect perfect results in terms of expectations being identical to the \"true\" theta and beta. The focus is on correctly implementing the SVI algorithm and observing its convergence behavior compared to CAVI.\n",
    "In general, SVI is faster and scales better to larger datasets compared to CAVI, but may converge to a less accurate solution in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Generate data\n",
    "The cell below generates data for the LDA model. Note, for simplicity, we are using N_d = N for all d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average z of each document should be close to theta of document. \n",
      " Theta of doc 0: [0.44432458 0.55567542] \n",
      " Mean z of doc 0: [0.36 0.64]\n",
      "Beta of topic 0: [0.00509409 0.63746039 0.24652082 0.         0.1109247 ]\n",
      "Beta of topic 1: [0.07322534 0.42700516 0.39144198 0.         0.10832752]\n",
      "Word to topic assignment, z, of document 0: [1 1 1 1 1 1 1 1 1 1]\n",
      "Observed words, w, of document 0: [0 1 0 4 2 1 1 1 2 2]\n",
      "Unique words and count of document 0: ['0: 2', '1: 23', '2: 18', '4: 7']\n"
     ]
    }
   ],
   "source": [
    "def generate_data(D, N, K, W, eta, alpha):\n",
    "    # sample K topics\n",
    "    beta = sp_stats.dirichlet(eta).rvs(size=K)  # size K x W\n",
    "\n",
    "    theta = np.zeros((D, K))   # size D x K\n",
    "\n",
    "    w = np.zeros((D, N, W))\n",
    "    z = np.zeros((D, N), dtype=int)\n",
    "    for d in range(D):\n",
    "        # sample document topic distribution\n",
    "        theta_d = sp_stats.dirichlet(alpha).rvs(size=1)\n",
    "        theta[d] = theta_d\n",
    "        for n in range(N):\n",
    "            # sample word to topic assignment\n",
    "            z_nd = sp_stats.multinomial(n=1, p=theta[d, :]).rvs(size=1).argmax(axis=1)[0]\n",
    "\n",
    "            # sample word\n",
    "            w_nd = sp_stats.multinomial(n=1, p=beta[z_nd, :]).rvs(1)\n",
    "\n",
    "            z[d, n] = z_nd\n",
    "            w[d, n] = w_nd\n",
    "\n",
    "    return w, z, theta, beta\n",
    "\n",
    "D_sim = 500\n",
    "N_sim = 50\n",
    "K_sim = 2\n",
    "\n",
    "W_sim = 5\n",
    "\n",
    "eta_sim = np.ones(W_sim)\n",
    "eta_sim[3] = 0.0001  # Expect word 3 to not appear in data\n",
    "eta_sim[1] = 3.  # Expect word 1 to be most common in data\n",
    "alpha_sim = np.ones(K_sim) * 1.0\n",
    "w0, z0, theta0, beta0 = generate_data(D_sim, N_sim, K_sim, W_sim, eta_sim, alpha_sim)\n",
    "w_cat = w0.argmax(axis=-1)  # remove one hot encoding\n",
    "unique_z, counts_z = numpy.unique(z0[0, :], return_counts=True)\n",
    "unique_w, counts_w = numpy.unique(w_cat[0, :], return_counts=True)\n",
    "\n",
    "# Sanity checks for data generation\n",
    "\n",
    "print(f\"Average z of each document should be close to theta of document. \\n Theta of doc 0: {theta0[0]} \\n Mean z of doc 0: {counts_z/N_sim}\")\n",
    "print(f\"Beta of topic 0: {beta0[0]}\")\n",
    "print(f\"Beta of topic 1: {beta0[1]}\")\n",
    "print(f\"Word to topic assignment, z, of document 0: {z0[0, 0:10]}\")\n",
    "print(f\"Observed words, w, of document 0: {w_cat[0, 0:10]}\")\n",
    "print(f\"Unique words and count of document 0: {[f'{u}: {c}' for u, c in zip(unique_w, counts_w)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version 2.9.1+cu128 loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Force the threading layer to be compatible with Jupyter on macOS\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "print(f\"Torch version {torch.__version__} loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as t_dist\n",
    "\n",
    "def generate_data_torch(D, N, K, W, eta, alpha):\n",
    "    \"\"\"\n",
    "    Torch implementation for generating data using the LDA model. Needed for sampling larger datasets.\n",
    "    \"\"\"\n",
    "    # sample K topics\n",
    "    beta_dist = t_dist.Dirichlet(torch.from_numpy(eta))\n",
    "    beta = beta_dist.sample([K])  # size K x W\n",
    "\n",
    "    # sample document topic distribution\n",
    "    theta_dist = t_dist.Dirichlet(torch.from_numpy(alpha))\n",
    "    theta = theta_dist.sample([D])\n",
    "\n",
    "    # sample word to topic assignment\n",
    "    z_dist = t_dist.OneHotCategorical(probs=theta)\n",
    "    z = z_dist.sample([N])\n",
    "    z = torch.einsum(\"ndk->dnk\", z)\n",
    "\n",
    "    # sample word from selected topics\n",
    "    beta_select = torch.einsum(\"kw, dnk -> dnw\", beta, z)\n",
    "    w_dist = t_dist.OneHotCategorical(probs=beta_select)\n",
    "    w = w_dist.sample([1])\n",
    "\n",
    "    w = w.reshape(D, N, W)\n",
    "\n",
    "    return w.numpy(), z.numpy(), theta.numpy(), beta.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def log_multivariate_beta_function(a, axis=None):\n",
    "    return np.sum(sp_spec.gammaln(a)) - sp_spec.gammaln(np.sum(a, axis=axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### CAVI Implementation, ELBO and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO: -27174.98: 100%|██████████| 100/100 [00:12<00:00,  7.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def initialize_q(w, D, N, K, W):\n",
    "    \"\"\"\n",
    "    Random initialization.\n",
    "    \"\"\"\n",
    "    phi_init = np.random.random(size=(D, N, K))\n",
    "    phi_init = phi_init / np.sum(phi_init, axis=-1, keepdims=True)\n",
    "    gamma_init = np.random.randint(1, 10, size=(D, K))\n",
    "    lmbda_init = np.random.randint(1, 10, size=(K, W))\n",
    "    return phi_init, gamma_init, lmbda_init\n",
    "\n",
    "def update_q_Z(w, gamma, lmbda):\n",
    "    D, N, W = w.shape\n",
    "    K, W = lmbda.shape\n",
    "    E_log_theta = sp_spec.digamma(gamma) - sp_spec.digamma(np.sum(gamma, axis=1, keepdims=True))  # size D x K\n",
    "    E_log_beta = sp_spec.digamma(lmbda) - sp_spec.digamma(np.sum(lmbda, axis=1, keepdims=True))   # size K x W\n",
    "    log_rho = np.zeros((D, N, K))\n",
    "    w_label = w.argmax(axis=-1)\n",
    "    for d in range(D):\n",
    "        for n in range(N):\n",
    "            E_log_beta_wdn = E_log_beta[:, int(w_label[d, n])]\n",
    "            E_log_theta_d = E_log_theta[d]\n",
    "            log_rho_n = E_log_theta_d + E_log_beta_wdn\n",
    "            log_rho[d, n, :] = log_rho_n\n",
    "\n",
    "    phi = np.exp(log_rho - sp_spec.logsumexp(log_rho, axis=-1, keepdims=True))\n",
    "    return phi\n",
    "\n",
    "def update_q_theta(phi, alpha):\n",
    "    E_Z = phi\n",
    "    D, N, K = phi.shape\n",
    "    gamma = np.zeros((D, K))\n",
    "    for d in range(D):\n",
    "        E_Z_d = E_Z[d]\n",
    "        gamma[d] = alpha + np.sum(E_Z_d, axis=0)  # sum over N\n",
    "    return gamma\n",
    "\n",
    "def update_q_beta(w, phi, eta):\n",
    "    E_Z = phi\n",
    "    D, N, W = w.shape\n",
    "    K = phi.shape[-1]\n",
    "    lmbda = np.zeros((K, W))\n",
    "    for k in range(K):\n",
    "        lmbda[k, :] = eta\n",
    "        for d in range(D):\n",
    "            for n in range(N):\n",
    "                lmbda[k, :] += E_Z[d,n,k] * w[d,n]  # Sum over d and n\n",
    "    return lmbda\n",
    "\n",
    "def calculate_elbo(w, phi, gamma, lmbda, eta, alpha):\n",
    "    D, N, K = phi.shape\n",
    "    W = eta.shape[0]\n",
    "    E_log_theta = sp_spec.digamma(gamma) - sp_spec.digamma(np.sum(gamma, axis=1, keepdims=True))  # size D x K\n",
    "    E_log_beta = sp_spec.digamma(lmbda) - sp_spec.digamma(np.sum(lmbda, axis=1, keepdims=True))  # size K x W\n",
    "    E_Z = phi  # size D, N, K\n",
    "    log_Beta_alpha = log_multivariate_beta_function(alpha)\n",
    "    log_Beta_eta = log_multivariate_beta_function(eta)\n",
    "    log_Beta_gamma = np.array([log_multivariate_beta_function(gamma[d, :]) for d in range(D)])\n",
    "    dg_gamma = sp_spec.digamma(gamma)\n",
    "    log_Beta_lmbda = np.array([log_multivariate_beta_function(lmbda[k, :]) for k in range(K)])\n",
    "    dg_lmbda = sp_spec.digamma(lmbda)\n",
    "\n",
    "    neg_CE_likelihood = np.einsum(\"dnk, kw, dnw\", E_Z, E_log_beta, w)\n",
    "    neg_CE_Z = np.einsum(\"dnk, dk -> \", E_Z, E_log_theta)\n",
    "    neg_CE_theta = -D * log_Beta_alpha + np.einsum(\"k, dk ->\", alpha - 1, E_log_theta)\n",
    "    neg_CE_beta = -K * log_Beta_eta + np.einsum(\"w, kw ->\", eta - 1, E_log_beta)\n",
    "    H_Z = -np.einsum(\"dnk, dnk ->\", E_Z, np.log(E_Z))\n",
    "    gamma_0 = np.sum(gamma, axis=1)\n",
    "    dg_gamma0 = sp_spec.digamma(gamma_0)\n",
    "    H_theta = np.sum(log_Beta_gamma + (gamma_0 - K) * dg_gamma0 - np.einsum(\"dk, dk -> d\", gamma - 1, dg_gamma))\n",
    "    lmbda_0 = np.sum(lmbda, axis=1)\n",
    "    dg_lmbda0 = sp_spec.digamma(lmbda_0)\n",
    "    H_beta = np.sum(log_Beta_lmbda + (lmbda_0 - W) * dg_lmbda0 - np.einsum(\"kw, kw -> k\", lmbda - 1, dg_lmbda))\n",
    "    return neg_CE_likelihood + neg_CE_Z + neg_CE_theta + neg_CE_beta + H_Z + H_theta + H_beta\n",
    "\n",
    "def CAVI_algorithm(w, K, n_iter, eta, alpha):\n",
    "  D, N, W = w.shape\n",
    "  phi, gamma, lmbda = initialize_q(w, D, N, K, W)\n",
    "\n",
    "  # Store output per iteration\n",
    "  elbo = np.zeros(n_iter)\n",
    "  phi_out = np.zeros((n_iter, D, N, K))\n",
    "  gamma_out = np.zeros((n_iter, D, K))\n",
    "  lmbda_out = np.zeros((n_iter, K, W))\n",
    "  \n",
    "  pbar = tqdm.tqdm(range(n_iter))\n",
    "  for i in pbar:\n",
    "\n",
    "    ###### CAVI updates #######\n",
    "\n",
    "    # q(Z) update\n",
    "    phi = update_q_Z(w, gamma, lmbda)\n",
    "\n",
    "    # q(theta) update\n",
    "    gamma = update_q_theta(phi, alpha)\n",
    "\n",
    "    # q(beta) update\n",
    "    lmbda = update_q_beta(w, phi, eta)\n",
    "\n",
    "    # ELBO\n",
    "    elbo[i] = calculate_elbo(w, phi, gamma, lmbda, eta, alpha)\n",
    "    \n",
    "    # outputs\n",
    "    phi_out[i] = phi\n",
    "    gamma_out[i] = gamma\n",
    "    lmbda_out[i] = lmbda\n",
    "    \n",
    "    pbar.set_description(f\"ELBO: {elbo[i]:.2f}\")\n",
    "\n",
    "  return phi_out, gamma_out, lmbda_out, elbo\n",
    "\n",
    "n_iter0 = 100\n",
    "K0 = K_sim\n",
    "W0 = W_sim\n",
    "eta_prior0 = np.ones(W0)\n",
    "alpha_prior0 = np.ones(K0)\n",
    "phi_out0, gamma_out0, lmbda_out0, elbo0 = CAVI_algorithm(w0, K0, n_iter0, eta_prior0, alpha_prior0)\n",
    "final_phi0 = phi_out0[-1]\n",
    "final_gamma0 = gamma_out0[-1]\n",
    "final_lmbda0 = lmbda_out0[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Recall label switching - compare E[theta] and true theta and check for label switching -----\n",
      "Final E[theta] of doc 0 CAVI:  [0.448 0.552]\n",
      "True theta of doc 0:          [0.444 0.556]\n",
      "----- Recall label switching - e.g. E[beta_0] could be fit to true theta_1. -----\n",
      "Final E[beta] k=0: [0.    0.733 0.159 0.    0.109]\n",
      "Final E[beta] k=1: [0.087 0.271 0.524 0.    0.119]\n",
      "True beta k=0: [0.005 0.637 0.247 0.    0.111]\n",
      "True beta k=1: [0.073 0.427 0.391 0.    0.108]\n"
     ]
    }
   ],
   "source": [
    "precision = 3\n",
    "print(f\"----- Recall label switching - compare E[theta] and true theta and check for label switching -----\")\n",
    "print(f\"Final E[theta] of doc 0 CAVI:  {np.round(final_gamma0[0] / np.sum(final_gamma0[0], axis=0, keepdims=True), precision)}\")\n",
    "print(f\"True theta of doc 0:          {np.round(theta0[0], precision)}\")\n",
    "\n",
    "print(f\"----- Recall label switching - e.g. E[beta_0] could be fit to true theta_1. -----\")\n",
    "print(f\"Final E[beta] k=0: {np.round(final_lmbda0[0, :] / np.sum(final_lmbda0[0, :], axis=-1, keepdims=True), precision)}\")\n",
    "print(f\"Final E[beta] k=1: {np.round(final_lmbda0[1, :] / np.sum(final_lmbda0[1, :], axis=-1, keepdims=True), precision)}\")\n",
    "print(f\"True beta k=0: {np.round(beta0[0, :], precision)}\")\n",
    "print(f\"True beta k=1: {np.round(beta0[1, :], precision)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### SVI Implementation\n",
    "\n",
    "Using the CAVI updates as a template, finish the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1835990985.py, line 67)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"phi: \" phi.shape)\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"def update_q_Z(w, gamma, lmbda):\n",
    "    D, N, W = w.shape\n",
    "    K, W = lmbda.shape\n",
    "    E_log_theta = sp_spec.digamma(gamma) - sp_spec.digamma(np.sum(gamma, axis=1, keepdims=True))  # size D x K\n",
    "    E_log_beta = sp_spec.digamma(lmbda) - sp_spec.digamma(np.sum(lmbda, axis=1, keepdims=True))   # size K x W\n",
    "    log_rho = np.zeros((D, N, K))\n",
    "    w_label = w.argmax(axis=-1)\n",
    "    for d in range(D):\n",
    "        for n in range(N):\n",
    "            E_log_beta_wdn = E_log_beta[:, int(w_label[d, n])]\n",
    "            E_log_theta_d = E_log_theta[d]\n",
    "            log_rho_n = E_log_theta_d + E_log_beta_wdn\n",
    "            log_rho[d, n, :] = log_rho_n\n",
    "\n",
    "    phi = np.exp(log_rho - sp_spec.logsumexp(log_rho, axis=-1, keepdims=True))\n",
    "    return phi\n",
    "\n",
    "def update_q_theta(phi, alpha):\n",
    "    E_Z = phi\n",
    "    D, N, K = phi.shape\n",
    "    gamma = np.zeros((D, K))\n",
    "    for d in range(D):\n",
    "        E_Z_d = E_Z[d]\n",
    "        gamma[d] = alpha + np.sum(E_Z_d, axis=0)  # sum over N\n",
    "    return gamma\n",
    "\n",
    "def update_q_beta(w, phi, eta):\n",
    "    E_Z = phi\n",
    "    D, N, W = w.shape\n",
    "    K = phi.shape[-1]\n",
    "    lmbda = np.zeros((K, W))\n",
    "    for k in range(K):\n",
    "        lmbda[k, :] = eta\n",
    "        for d in range(D):\n",
    "            for n in range(N):\n",
    "                lmbda[k, :] += E_Z[d,n,k] * w[d,n]  # Sum over d and n\n",
    "    return lmbda\n",
    "\"\"\"\n",
    "\n",
    "def update_q_Z_svi(batch, w, gamma, lmbda):\n",
    "    \"\"\"\n",
    "    TODO: rewrite CAVI update to SVI update\n",
    "    \"\"\"\n",
    "    w_batch = w[batch, :, :]\n",
    "    D, N, W = w_batch.shape\n",
    "    K, W = lmbda.shape\n",
    "    E_log_theta = sp_spec.digamma(gamma) - sp_spec.digamma(np.sum(gamma, axis=1, keepdims=True))  # size D x K\n",
    "    E_log_beta = sp_spec.digamma(lmbda) - sp_spec.digamma(np.sum(lmbda, axis=1, keepdims=True))   # size K x W\n",
    "    log_rho = np.zeros((D, N, K))\n",
    "    w_label = w_batch.argmax(axis=-1)\n",
    "    for d in range(D):\n",
    "        for n in range(N):\n",
    "            E_log_beta_wdn = E_log_beta[:, int(w_label[d, n])]\n",
    "            E_log_theta_d = E_log_theta[d]\n",
    "            log_rho_n = E_log_theta_d + E_log_beta_wdn\n",
    "            log_rho[d, n, :] = log_rho_n\n",
    "\n",
    "    phi_batch = np.exp(log_rho - sp_spec.logsumexp(log_rho, axis=-1, keepdims=True))\n",
    "\n",
    "    return phi_batch\n",
    "\n",
    "def update_q_theta_svi(batch, phi, alpha):\n",
    "    \"\"\"\n",
    "    TODO: rewrite CAVI update to SVI update\n",
    "    \"\"\"\n",
    "    print(\"batch: \", batch)\n",
    "    print(\"phi: \", phi.shape)\n",
    "    phi_batch = phi[batch, :, :]\n",
    "    E_Z = phi_batch\n",
    "    D, N, K = phi_batch.shape\n",
    "    gamma_batch = np.zeros((D, K))\n",
    "    for d in range(D):\n",
    "        E_Z_d = E_Z[d]\n",
    "        gamma_batch[d] = alpha + np.sum(E_Z_d, axis=0)  # sum over N\n",
    "    return gamma_batch\n",
    "\n",
    "def update_q_beta_svi(batch, w, phi, eta):\n",
    "    \"\"\"\n",
    "    TODO: rewrite CAVI update to SVI update\n",
    "    \"\"\"\n",
    "    phi_batch = phi[batch, :, :]\n",
    "    w_batch = w[batch, :, :]\n",
    "    E_Z = phi_batch\n",
    "    D, N, W = w_batch.shape\n",
    "    K = phi_batch.shape[-1]\n",
    "    lmbda_batch = np.zeros((K, W))\n",
    "    for k in range(K):\n",
    "        lmbda_batch[k, :] = eta\n",
    "        for d in range(D):\n",
    "            for n in range(N):\n",
    "                lmbda_batch[k, :] += E_Z[d,n,k] * w_batch[d,n]  # Sum over d and n\n",
    "    return lmbda_batch\n",
    "\n",
    "def SVI_algorithm(w, K, S, n_iter, eta, alpha):\n",
    "  \"\"\"\n",
    "  Add SVI Specific code here.\n",
    "  \"\"\"\n",
    "  D, N, W = w.shape\n",
    "  phi, gamma, lmbda = initialize_q(w, D, N, K, W)\n",
    "\n",
    "  # Store output per iteration\n",
    "  elbo = np.zeros(n_iter)\n",
    "  phi_out = np.zeros((n_iter, D, N, K))\n",
    "  gamma_out = np.zeros((n_iter, D, K))\n",
    "  lmbda_out = np.zeros((n_iter, K, W))\n",
    "\n",
    "  delay = n_iter // 2 + 1\n",
    "  forgetting_rate = 0.9\n",
    "  pbar = tqdm.tqdm(range(n_iter))\n",
    "  for t in pbar:\n",
    "    \n",
    "\n",
    "    ###### SVI updates - following figure 6 in Hoffman paper #######\n",
    "\n",
    "    # Sample batch and set step size, rho.\n",
    "    rho = (delay + t) ** (-forgetting_rate)\n",
    "\n",
    "    ### Update locals on sampled batch_d until converge ###\n",
    "    converge = False\n",
    "    batch_d = np.random.randint(0, D, size=S)\n",
    "\n",
    "    ###### SVI updates #######\n",
    "    i = 0\n",
    "    gamma[batch_d, :] = 1. #np.random.randint(1, 10, size=K)\n",
    "    gamma_prev = np.zeros((S, N, K))\n",
    "    phi_prev = np.zeros((S, N, K))\n",
    "    while not converge:\n",
    "        ## Update local variational parameters until convergence ##\n",
    "        \n",
    "        ###### CAVI updates #######\n",
    "\n",
    "        # q(Z) update\n",
    "        phi[batch_d] = update_q_Z_svi(batch_d, w, gamma, lmbda)\n",
    "\n",
    "        # q(theta) update\n",
    "        gamma[batch_d] = update_q_theta_svi(batch_d, phi, alpha)\n",
    "\n",
    "        # q(beta) update\n",
    "        lmbda[batch_d] = update_q_beta_svi(batch_d, w, phi, eta)\n",
    "\n",
    "        # converge condition\n",
    "        i += 1\n",
    "        if (np.sum(np.abs(gamma_prev - gamma[batch_d])) < S*0.1 and \\\n",
    "                np.sum(np.abs(phi_prev - phi[batch_d])) < S*0.1) or i > 50:\n",
    "            converge = True\n",
    "        gamma_prev = gamma[batch_d]\n",
    "        phi_prev = phi[batch_d]\n",
    "\n",
    "    ### Update globals ###\n",
    "\n",
    "    # ELBO\n",
    "    elbo[t] = calculate_elbo(w, phi, gamma, lmbda, eta, alpha)\n",
    "\n",
    "    # outputs\n",
    "    phi_out[t] = phi\n",
    "    gamma_out[t] = gamma\n",
    "    lmbda_out[t] = lmbda\n",
    "      \n",
    "    pbar.set_description(f\"ELBO: {elbo[t]:.2f}\")\n",
    "\n",
    "  return phi_out, gamma_out, lmbda_out, elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### CASE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAVI start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO: -3870.73: 100%|██████████| 100/100 [00:02<00:00, 35.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVI start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m start_svi1 = time.time()\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSVI start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m phi_out1_svi, gamma_out1_svi, lmbda_out1_svi, elbo1_svi = \u001b[43mSVI_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter_svi1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_prior1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_prior1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m end_svi1 = time.time()\n\u001b[32m     30\u001b[39m final_phi1_cavi = phi_out1_cavi[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mSVI_algorithm\u001b[39m\u001b[34m(w, K, S, n_iter, eta, alpha)\u001b[39m\n\u001b[32m    128\u001b[39m phi = update_q_Z_svi(batch_d, w, gamma, lmbda)\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# q(theta) update\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m gamma = \u001b[43mupdate_q_theta_svi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# q(beta) update\u001b[39;00m\n\u001b[32m    134\u001b[39m lmbda = update_q_beta_svi(batch_d, w, phi, eta)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mupdate_q_theta_svi\u001b[39m\u001b[34m(batch, phi, alpha)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_q_theta_svi\u001b[39m(batch, phi, alpha):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m    TODO: rewrite CAVI update to SVI update\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     phi_batch = \u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     67\u001b[39m     E_Z = phi_batch\n\u001b[32m     68\u001b[39m     D, N, K = phi_batch.shape\n",
      "\u001b[31mIndexError\u001b[39m: index 12 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Data simulation parameters\n",
    "D1 = 50\n",
    "N1 = 50\n",
    "K1 = 2\n",
    "W1 = 5\n",
    "eta_sim1 = np.ones(W1)\n",
    "alpha_sim1 = np.ones(K1)\n",
    "\n",
    "w1, z1, theta1, beta1 = generate_data(D1, N1, K1, W1, eta_sim1, alpha_sim1)\n",
    "\n",
    "# Inference parameters\n",
    "n_iter_cavi1 = 100\n",
    "n_iter_svi1 = 100\n",
    "eta_prior1 = np.ones(W1) * 1.\n",
    "alpha_prior1 = np.ones(K1) * 1.\n",
    "S1 = N1 // 10 # batch size\n",
    "\n",
    "start_cavi1 = time.time()\n",
    "print(\"CAVI start\")\n",
    "phi_out1_cavi, gamma_out1_cavi, lmbda_out1_cavi, elbo1_cavi = CAVI_algorithm(w1, K1, n_iter_cavi1, eta_prior1, alpha_prior1)\n",
    "end_cavi1 = time.time()\n",
    "\n",
    "start_svi1 = time.time()\n",
    "print(\"SVI start\")\n",
    "phi_out1_svi, gamma_out1_svi, lmbda_out1_svi, elbo1_svi = SVI_algorithm(w1, K1, S1, n_iter_svi1, eta_prior1, alpha_prior1)\n",
    "end_svi1 = time.time()\n",
    "\n",
    "final_phi1_cavi = phi_out1_cavi[-1]\n",
    "final_gamma1_cavi = gamma_out1_cavi[-1]\n",
    "final_lmbda1_cavi = lmbda_out1_cavi[-1]\n",
    "final_phi1_svi = phi_out1_svi[-1]\n",
    "final_gamma1_svi = gamma_out1_svi[-1]\n",
    "final_lmbda1_svi = lmbda_out1_svi[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Evaluation\n",
    "Do not expect perfect results in terms expectations being identical to the \"true\" theta and beta.\n",
    "Do not expect the ELBO plot of your SVI alg to be the same as the CAVI alg. However, it should increase and be in the same ball park as that of the CAVI alg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print(f\"----- Recall label switching - compare E[theta] and true theta and check for label switching -----\")\n",
    "print(f\"E[theta] of doc 0 SVI:  {final_gamma1_svi[0] / np.sum(final_gamma1_svi[0], axis=0, keepdims=True)}\")\n",
    "print(f\"E[theta] of doc 0 CAVI: {final_gamma1_cavi[0] / np.sum(final_gamma1_cavi[0], axis=0, keepdims=True)}\")\n",
    "print(f\"True theta of doc 0:    {theta1[0]}\")\n",
    "\n",
    "print(f\"----- Recall label switching - e.g. E[beta_0] could be fit to true theta_1. -----\")\n",
    "print(f\"E[beta] SVI k=0:    {final_lmbda1_svi[0, :] / np.sum(final_lmbda1_svi[0, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] SVI k=1:    {final_lmbda1_svi[1, :] / np.sum(final_lmbda1_svi[1, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] CAVI k=0:   {final_lmbda1_cavi[0, :] / np.sum(final_lmbda1_cavi[0, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] CAVI k=1:   {final_lmbda1_cavi[1, :] / np.sum(final_lmbda1_cavi[1, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"True beta k=0:      {beta1[0, :]}\")\n",
    "print(f\"True beta k=1:      {beta1[1, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add your own code for evaluation here (will not be graded)\n",
    "print(f\"Time SVI: {end_svi1 - start_svi1}\")\n",
    "print(f\"Time CAVI: {end_cavi1 - start_cavi1}\")\n",
    "\n",
    "plt.plot(list(range(1, n_iter_cavi1 + 1)), elbo1_svi[np.arange(0, n_iter_svi1, int(n_iter_svi1 / n_iter_cavi1))])\n",
    "plt.plot(list(range(1, n_iter_cavi1 + 1)), elbo1_cavi)\n",
    "plt.title(\"ELBO plot\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ELBO SVI: {elbo1_svi[-1]}\")\n",
    "print(f\"ELBO CAVI: {elbo1_cavi[-1]}\")\n",
    "print(f\"ELBO difference: {elbo1_svi[-1] - elbo1_cavi[-1]}\")\n",
    "print(f\"ELBO quotient: {elbo1_svi[-1]/elbo1_cavi[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### CASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Data simulation parameters\n",
    "D2 = 1000\n",
    "N2 = 50\n",
    "K2 = 3\n",
    "W2 = 10\n",
    "eta_sim2 = np.ones(W2)\n",
    "alpha_sim2 = np.ones(K2)\n",
    "\n",
    "w2, z2, theta2, beta2 = generate_data(D2, N2, K2, W2, eta_sim2, alpha_sim2)\n",
    "\n",
    "# Inference parameters\n",
    "n_iter_cavi2 = 100\n",
    "n_iter_svi2 = 100\n",
    "eta_prior2 = np.ones(W2) * 1.\n",
    "alpha_prior2 = np.ones(K2) * 1.\n",
    "S2 = D2 // 10 # batch size\n",
    "\n",
    "start_cavi2 = time.time()\n",
    "phi_out2_cavi, gamma_out2_cavi, lmbda_out2_cavi, elbo2_cavi = CAVI_algorithm(w2, K2, n_iter_cavi2, eta_prior2, alpha_prior2)\n",
    "end_cavi2 = time.time()\n",
    "\n",
    "start_svi2 = time.time()\n",
    "phi_out2_svi, gamma_out2_svi, lmbda_out2_svi, elbo2_svi = SVI_algorithm(w2, K2, S2, n_iter_svi2, eta_prior2, alpha_prior2)\n",
    "end_svi2 = time.time()\n",
    "\n",
    "final_phi2_cavi = phi_out2_cavi[-1]\n",
    "final_gamma2_cavi = gamma_out2_cavi[-1]\n",
    "final_lmbda2_cavi = lmbda_out2_cavi[-1]\n",
    "final_phi2_svi = phi_out2_svi[-1]\n",
    "final_gamma2_svi = gamma_out2_svi[-1]\n",
    "final_lmbda2_svi = lmbda_out2_svi[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "    #### Evaluation\n",
    "Do not expect perfect results in terms expectations being identical to the \"true\" theta and beta.\n",
    "Do not expect the ELBO plot of your SVI alg to be the same as the CAVI alg. However, it should increase and be in the same ball park as that of the CAVI alg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "print(f\"----- Recall label switching - compare E[theta] and true theta and check for label switching -----\")\n",
    "print(f\"E[theta] of doc 0 SVI:      {final_gamma2_svi[0] / np.sum(final_gamma2_svi[0], axis=0, keepdims=True)}\")\n",
    "print(f\"E[theta] of doc 0 CAVI:     {final_gamma2_cavi[0] / np.sum(final_gamma2_cavi[0], axis=0, keepdims=True)}\")\n",
    "print(f\"True theta of doc 0:        {theta2[0]}\")\n",
    "\n",
    "print(f\"----- Recall label switching - e.g. E[beta_0] could be fit to true theta_1. -----\")\n",
    "print(f\"E[beta] k=0:    {final_lmbda2_svi[0, :] / np.sum(final_lmbda2_svi[0, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] k=1:    {final_lmbda2_svi[1, :] / np.sum(final_lmbda2_svi[1, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"E[beta] k=2:    {final_lmbda2_svi[2, :] / np.sum(final_lmbda2_svi[2, :], axis=-1, keepdims=True)}\")\n",
    "print(f\"True beta k=0:  {beta2[0, :]}\")\n",
    "print(f\"True beta k=1:  {beta2[1, :]}\")\n",
    "print(f\"True beta k=2:  {beta2[2, :]}\")\n",
    "\n",
    "print(f\"Time SVI: {end_svi2 - start_svi2}\")\n",
    "print(f\"Time CAVI: {end_cavi2 - start_cavi2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add your own code for evaluation here (will not be graded)\n",
    "plt.plot(list(range(1, n_iter_cavi2 + 1)), elbo2_svi[np.arange(0, n_iter_svi2, int(n_iter_svi2 / n_iter_cavi2))])\n",
    "plt.plot(list(range(1, n_iter_cavi2 + 1)), elbo2_cavi)\n",
    "plt.title(\"ELBO plot\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ELBO SVI: {elbo2_svi[-1]}\")\n",
    "print(f\"ELBO CAVI: {elbo2_cavi[-1]}\")\n",
    "print(f\"ELBO difference: {elbo2_svi[-1] - elbo2_cavi[-1]}\")\n",
    "print(f\"ELBO quotient: {elbo2_svi[-1]/elbo2_cavi[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### CASE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Data simulation parameters\n",
    "D3 = 10**3\n",
    "N3 = 1000\n",
    "K3 = 10\n",
    "W3 = 100\n",
    "eta_sim3 = np.ones(W3)\n",
    "alpha_sim3 = np.ones(K3)\n",
    "\n",
    "w3, z3, theta3, beta3 = generate_data_torch(D3, N3, K3, W3, eta_sim3, alpha_sim3)\n",
    "\n",
    "# Inference parameters\n",
    "n_iter3 = 1\n",
    "eta_prior3 = np.ones(W3) * 1.\n",
    "alpha_prior3 = np.ones(K3) * 1.\n",
    "S3 = D3 // 10 # batch size\n",
    "\n",
    "start_cavi3 = time.time()\n",
    "phi_out3_cavi, gamma_out3_cavi, lmbda_out3_cavi, elbo3_cavi = CAVI_algorithm(w3, K3, n_iter3, eta_prior3, alpha_prior3)\n",
    "end_cavi3 = time.time()\n",
    "\n",
    "start_svi3 = time.time()\n",
    "phi_out3_svi, gamma_out3_svi, lmbda_out3_svi, elbo3_svi = SVI_algorithm(w3, K3, S3, n_iter3, eta_prior3, alpha_prior3)\n",
    "end_svi3 = time.time()\n",
    "\n",
    "final_phi3_cavi = phi_out3_cavi[-1]\n",
    "final_gamma3_cavi = gamma_out3_cavi[-1]\n",
    "final_lmbda3_cavi = lmbda_out3_cavi[-1]\n",
    "final_phi3_svi = phi_out3_svi[-1]\n",
    "final_gamma3_svi = gamma_out3_svi[-1]\n",
    "final_lmbda3_svi = lmbda_out3_svi[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add your own code for evaluation here (will not be graded)\n",
    "print(f\"Time SVI: {end_svi3 - start_svi3}\")\n",
    "print(f\"Time CAVI: {end_cavi3 - start_cavi3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
