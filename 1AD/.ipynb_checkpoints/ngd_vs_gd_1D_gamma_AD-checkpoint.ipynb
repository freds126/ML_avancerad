{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d1cc2d036e890d",
   "metadata": {},
   "source": [
    "# Q 2.4.17. Standard Gradient Descent vs. Natural Gradient Descent\n",
    "In this notebbook, we compare standard gradient descent (GD) and natural gradient descent (NGD) for estimating the parameters of a 1D Gamma distribution from observed data. We will generate synthetic data from a known Gamma distribution and then attempt to recover its shape and scale parameters using both optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ecc5079214231",
   "metadata": {},
   "source": [
    "# 1. Configuration\n",
    "We define the true parameters of the Gamma distribution, the number of data points to generate, the learning rate, the number of epochs for optimization, and the initial guesses for the parameters. We also set a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c1adc14b355c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Gamma\n",
    "# ------------------------\n",
    "\n",
    "# True parameters of the Gamma distribution we want to discover\n",
    "ALPHA_TRUE = 3.0   # shape\n",
    "BETA_TRUE  = 2.0   # scale\n",
    "\n",
    "# Number of observed data points\n",
    "N_DATA = 1000\n",
    "\n",
    "# Optimization parameters\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 150\n",
    "\n",
    "# Initial \"wrong\" guess for our parameters (still positive)\n",
    "ALPHA_INIT = 0.5\n",
    "BETA_INIT  = 8.0\n",
    "\n",
    "# Fix random seed for reproducibility (optional)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d323836a2fe74",
   "metadata": {},
   "source": [
    "# 2. Data Generation\n",
    "We generate synthetic data from the true Gamma distribution using PyTorch's `Gamma` distribution class. We sample `N_DATA` points and print the sample mean and variance to verify the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511b218a1abe489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our parameterisation is shape–scale, but PyTorch's Gamma uses shape–rate.\n",
    "rate_true = 1.0 / BETA_TRUE\n",
    "dist_true = Gamma(concentration=torch.tensor(ALPHA_TRUE),\n",
    "                  rate=torch.tensor(rate_true))\n",
    "\n",
    "data = dist_true.sample((N_DATA,))\n",
    "\n",
    "print(f\"Generated {N_DATA} data points from Gamma(alpha={ALPHA_TRUE}, beta={BETA_TRUE})\")\n",
    "print(f\"Sample mean:     {data.mean().item():.4f}\")\n",
    "print(f\"Sample variance: {data.var().item():.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a54819ea48bcc",
   "metadata": {},
   "source": [
    "# 3. Loss function & parameter init\n",
    "We define the negative log-likelihood loss function for the Gamma distribution. We also initialize the parameters for both standard gradient descent and natural gradient descent, and set up history trackers to record the parameter values over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80885c47364f9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Loss function\n",
    "def gamma_nll(alpha, beta, data_points):\n",
    "    \"\"\"\n",
    "    ToDo:\n",
    "        Implement the average negative log-likelihood for Gamma distribution with shape=alpha and scale=beta.\n",
    "\n",
    "        Hints:\n",
    "        - Enforce positivity using clamp (e.g. min=1e-4).\n",
    "        - PyTorch's Gamma takes (concentration=alpha, rate=1/beta).\n",
    "        - Return the *mean* negative log-likelihood.\n",
    "\n",
    "    \"\"\"\n",
    "    x_mean = torch.mean(data_points)\n",
    "    log_x_mean = torch.mean(torch.log(data_points))\n",
    "    \n",
    "    nll = gammaln(alpha) + alpha * np.log(beta) \\\n",
    "        - (alpha -1) * log_x_mean + x_mean / beta\n",
    "\n",
    "    return nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac5ce61c6fb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters for Standard Gradient Descent (GD)\n",
    "alpha_gd = torch.tensor(ALPHA_INIT, requires_grad=True)\n",
    "beta_gd  = torch.tensor(BETA_INIT,  requires_grad=True)\n",
    "\n",
    "# Parameters for Natural Gradient Descent (NGD)\n",
    "alpha_ngd = torch.tensor(ALPHA_INIT, requires_grad=True)\n",
    "beta_ngd  = torch.tensor(BETA_INIT,  requires_grad=True)\n",
    "\n",
    "# History trackers\n",
    "history_gd  = []\n",
    "history_ngd = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b011aed1e45c60",
   "metadata": {},
   "source": [
    "# 4. Fisher Information inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78f412721298b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_inverse(alpha, beta):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      Implement the inverse Fisher Information matrix F^{-1}(α, β)\n",
    "      for the Gamma(shape=α, scale=β) distribution.\n",
    "\n",
    "      Theory:\n",
    "        F(α, β) =\n",
    "            [ ψ1(α)        1/β      ]\n",
    "            [  1/β     α/β^2        ]\n",
    "\n",
    "        F^{-1}(α, β) =\n",
    "            1 / (α ψ1(α) - 1) *\n",
    "            [  α           -β             ]\n",
    "            [  -β       β^2 ψ1(α)         ]\n",
    "\n",
    "      Hints:\n",
    "      - Use torch.polygamma(1, alpha) for ψ1(α) (trigamma).\n",
    "      - Make sure to detach alpha, beta so F^{-1} is not part of the graph.\n",
    "    \"\"\"\n",
    "    alpha = alpha.detach()\n",
    "    beta = beta.detach()\n",
    "    \n",
    "    d_psi = torch.polygamma(1, alpha)\n",
    "    denom = alpha * d_psi - 1\n",
    "    \n",
    "    inv11 = alpha / denom\n",
    "    inv12 = -beta / denom\n",
    "    inv22 = d_psi * beta**2 / denom \n",
    "  \n",
    "    return inv11, inv12, inv22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dbd73577cd34b",
   "metadata": {},
   "source": [
    "# 4. Optimization Loop\n",
    "We run the optimization loop for a specified number of epochs. In each epoch, we perform both standard gradient descent and natural gradient descent updates. We compute the gradients, build the Fisher Information Matrix for NGD, and update the parameters accordingly. We also log the parameter values and losses at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a82b37f0862057",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LEARNING_RATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing with LR=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mLEARNING_RATE\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# ===== A. Standard Gradient Descent (GD) =====\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m alpha_gd\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LEARNING_RATE' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Optimizing with LR={LEARNING_RATE} for {EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # ===== A. Standard Gradient Descent (GD) =====\n",
    "\n",
    "    if alpha_gd.grad is not None:\n",
    "        alpha_gd.grad.zero_()\n",
    "    if beta_gd.grad is not None:\n",
    "        beta_gd.grad.zero_()\n",
    "\n",
    "    loss_gd = gamma_nll(alpha_gd, beta_gd, data)\n",
    "    loss_gd.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        alpha_gd -= LEARNING_RATE * alpha_gd.grad\n",
    "        beta_gd  -= LEARNING_RATE * beta_gd.grad\n",
    "\n",
    "        alpha_gd.clamp_(min=1e-4)\n",
    "        beta_gd.clamp_(min=1e-4)\n",
    "\n",
    "    history_gd.append((alpha_gd.item(), beta_gd.item()))\n",
    "\n",
    "    # ===== B. Natural Gradient Descent (NGD) =====\n",
    "\n",
    "    if alpha_ngd.grad is not None:\n",
    "        alpha_ngd.grad.zero_()\n",
    "    if beta_ngd.grad is not None:\n",
    "        beta_ngd.grad.zero_()\n",
    "\n",
    "    loss_ngd = gamma_nll(alpha_ngd, beta_ngd, data)\n",
    "    loss_ngd.backward()\n",
    "\n",
    "    g_alpha = alpha_ngd.grad\n",
    "    g_beta  = beta_ngd.grad\n",
    "\n",
    "    # ToDo : compute natural gradient using F^{-1}(α, β)\n",
    "    #  1) Get F^{-1} entries using fisher_inverse(...)\n",
    "    #  2) Compute:\n",
    "    #       - ng_alpha\n",
    "    #       - ng_beta\n",
    "\n",
    "    inv11, inv21, inv22 = fisher_inverse(alpha_ngd, beta_ngd)\n",
    "\n",
    "    ng_alpha =  inv11 * alpha_ngd + inv21 * beta_ngd\n",
    "    ng_beta  =  inv21 * alpha_ngd + inv22 * beta_ngd\n",
    "\n",
    "    with torch.no_grad():\n",
    "        alpha_ngd -= LEARNING_RATE * ng_alpha\n",
    "        beta_ngd  -= LEARNING_RATE * ng_beta\n",
    "\n",
    "        alpha_ngd.clamp_(min=1e-4)\n",
    "        beta_ngd.clamp_(min=1e-4)\n",
    "\n",
    "    history_ngd.append((alpha_ngd.item(), beta_ngd.item()))\n",
    "\n",
    "    if (epoch + 1) % 15 == 0 or epoch == 0:\n",
    "        print(f\"\\n--- Epoch {epoch + 1} ---\")\n",
    "        print(f\"  GD:  alpha={alpha_gd.item():.4f}, beta={beta_gd.item():.4f}, \"\n",
    "              f\"Loss={loss_gd.item():.4f}\")\n",
    "        print(f\"  NGD: alpha={alpha_ngd.item():.4f}, beta={beta_ngd.item():.4f}, \"\n",
    "              f\"Loss={loss_ngd.item():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nOptimization finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a5f27122c378f",
   "metadata": {},
   "source": [
    "# Q 2.4.18. Plotting Results\n",
    "To illustrate the difference between standard gradients and natural gradients, we print out the gradients computed in the first epoch for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51225312d5d3c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_gd_np  = np.array(history_gd)\n",
    "hist_ngd_np = np.array(history_ngd)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "fig.suptitle(f\"Gamma: Standard Gradient vs. Natural Gradient \"\n",
    "             f\"(LR={LEARNING_RATE}, N={N_DATA})\", fontsize=16)\n",
    "\n",
    "# Plot 1: alpha (shape)\n",
    "ax1.plot(hist_gd_np[:, 0],  label=\"GD alpha\",  color='blue', linestyle='--')\n",
    "ax1.plot(hist_ngd_np[:, 0], label=\"NGD alpha\", color='red')\n",
    "ax1.axhline(ALPHA_TRUE, color='black', linestyle=':', label=f\"True alpha ({ALPHA_TRUE})\")\n",
    "ax1.set_ylabel(\"Shape parameter $\\\\alpha$\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: beta (scale)\n",
    "ax2.plot(hist_gd_np[:, 1],  label=\"GD beta\",  color='blue', linestyle='--')\n",
    "ax2.plot(hist_ngd_np[:, 1], label=\"NGD beta\", color='red')\n",
    "ax2.axhline(BETA_TRUE, color='black', linestyle=':', label=f\"True beta ({BETA_TRUE})\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Scale parameter $\\\\beta$\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
